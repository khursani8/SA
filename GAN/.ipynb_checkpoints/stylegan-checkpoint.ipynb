{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier\"\"\"\n",
    "    def __init__(self,input_size, output_size,gain=2**(0.5), use_wscale=False,lrmul=1,bias=True):\n",
    "        super().__init__()\n",
    "        he_std = gain * input_size**(-0.5)\n",
    "        # Equalized learning rate and custom learning rate multiplier.\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_size,input_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "    def forward(self,x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "        return F.linear(x,self.weight * self.w_mul,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(nn.Module):\n",
    "    \"\"\"Conv layer with equalized lr and custom lr multiplier\"\"\"\n",
    "    def __init__(self,input_channels,output_channels,kernel_size,gain=2**(0.5),use_wscale=False, lrmul=1, bias=True,intermediate=None,upscale=False):\n",
    "        super().__init__()\n",
    "        if upscale:\n",
    "            self.upscale = Upscale2d()\n",
    "        else:\n",
    "            self.upscale = None\n",
    "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5)\n",
    "        self.kernel_size = kernel_size\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_channels,input_channels,kernel_size,kernel_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.intermediate = intermediate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "            \n",
    "        have_convolution = False\n",
    "        if self.upscale is not None and min(x.shape[2:]):\n",
    "            w = self.weight * self.w_mul\n",
    "            w = w.permute(1,0,2,3)\n",
    "            w = F.pad(w,(1,1,1,1))\n",
    "            w = w[:,:,1:,1:] + w[:,:,:-1,1:] + w[:,:,1:,:-1] + w[:,:,:-1,:-1]\n",
    "            x = F.conv_transpose2d(x,w,stride=2,padding=(w.size(-1)-1),have_convolution=True)\n",
    "        elif self.upscale is not None:\n",
    "            x = self.upscale(x)\n",
    "            \n",
    "        if not have_convolution and self.intermediate is None:\n",
    "            return F.conv2d(x,self.weight * self.w_mul,bias, padding=self.kernel_size//2)\n",
    "        elif not have_convolution:\n",
    "            x = F.conv2d(x,self.weight * self.w_mul, None, padding=self.kernel_size//2)\n",
    "            \n",
    "        if self.intermediate is not None:\n",
    "            x = self.intermediate(x)\n",
    "        if bias is not None:\n",
    "            x = x + bias.view(1,-1,1,1)\n",
    "        return x\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseLayer(nn.Module):\n",
    "    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n",
    "    def __init__(self,channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(channels))\n",
    "        self.noise = None\n",
    "    def forward(self,x,noise=None):\n",
    "        if noise is None and self.noise is None:\n",
    "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
    "        elif noise is None:\n",
    "            noise = self.noise\n",
    "        x = x + self.weight.view(1,-1,1,1) * noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleMod(nn.Module):\n",
    "    def __init__(self,latent_size,channels,use_wscale):\n",
    "        super(StyleMod,self).__init__()\n",
    "        self.lin = MyLinear(latent_size,channels*2,gain=1.0,use_wscale=use_wscale)\n",
    "        \n",
    "    def forward(self,x,latent):\n",
    "        style = self.lin(latent) # style => [batch_size,n_channels*2]\n",
    "        shape = [-1,2,x.size(1)] + (x.dim() - 2) * [1]\n",
    "        style = style.view(shape) # [batch_size,2,n_channels]\n",
    "        x = x * (style[:,0] + 1.) + style[:,1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormLayer(nn.Module): # forcing std for latent vector to be one\n",
    "    def __init__(self,epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self,x):\n",
    "        return x * torch.rsqrt(torch.mean(x**2,dim=1,keepdim=True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurLayer(nn.Module):\n",
    "    def __init__(self,kernel=[1,2,1],normalize=True,flip=False,stride=1):\n",
    "        super(BlurLayer,self).__init__()\n",
    "        kernel = [1,2,1]\n",
    "        kernel = torch.tensor(kernel,dtype=torch.float32)\n",
    "        kernel = kernel[:,None] * kernel[None,:]\n",
    "        kernel = kernel[None,None]\n",
    "        if normalize:\n",
    "            kernel = kernel / kernel.sum()\n",
    "        if flip:\n",
    "            kernel = kernel[:,:,::-1,::-1]\n",
    "        self.register_buffer('kernel',kernel)\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # expand kernels channels\n",
    "        kernel = self.kernel.expand(x.size(1),-1,-1,-1)\n",
    "        x = F.conv2d(x,kernel,stride=self.stride,padding=int((self.kernel.size(2)-1)/2),groups=x.size(1))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
