{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier\"\"\"\n",
    "    def __init__(self,input_size, output_size,gain=2**(0.5), use_wscale=False,lrmul=1,bias=True):\n",
    "        super().__init__()\n",
    "        he_std = gain * input_size**(-0.5)\n",
    "        # Equalized learning rate and custom learning rate multiplier.\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_size,input_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "    def forward(self,x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "        return F.linear(x,self.weight * self.w_mul,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(nn.Module):\n",
    "    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True,\n",
    "                intermediate=None, upscale=False):\n",
    "        super().__init__()\n",
    "        if upscale:\n",
    "            self.upscale = Upscale2d()\n",
    "        else:\n",
    "            self.upscale = None\n",
    "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5) # He init\n",
    "        self.kernel_size = kernel_size\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.intermediate = intermediate\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "        \n",
    "        have_convolution = False\n",
    "        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n",
    "            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n",
    "            # this really needs to be cleaned up and go into the conv...\n",
    "            w = self.weight * self.w_mul\n",
    "            w = w.permute(1, 0, 2, 3)\n",
    "            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n",
    "            w = F.pad(w, (1,1,1,1))\n",
    "            w = w[:, :, 1:, 1:]+ w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n",
    "            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1)-1)//2)\n",
    "            have_convolution = True\n",
    "        elif self.upscale is not None:\n",
    "            x = self.upscale(x)\n",
    "    \n",
    "        if not have_convolution and self.intermediate is None:\n",
    "            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size//2)\n",
    "        elif not have_convolution:\n",
    "            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size//2)\n",
    "        \n",
    "        if self.intermediate is not None:\n",
    "            x = self.intermediate(x)\n",
    "        if bias is not None:\n",
    "            x = x + bias.view(1, -1, 1, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseLayer(nn.Module):\n",
    "    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(channels))\n",
    "        self.noise = None\n",
    "    \n",
    "    def forward(self, x, noise=None):\n",
    "        if noise is None and self.noise is None:\n",
    "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
    "        elif noise is None:\n",
    "            # here is a little trick: if you get all the noiselayers and set each\n",
    "            # modules .noise attribute, you can have pre-defined noise.\n",
    "            # Very useful for analysis\n",
    "            noise = self.noise\n",
    "        x = x + self.weight.view(1, -1, 1, 1) * noise\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleMod(nn.Module):\n",
    "    def __init__(self,latent_size,channels,use_wscale):\n",
    "        super(StyleMod,self).__init__()\n",
    "        self.lin = MyLinear(latent_size,channels*2,gain=1.0,use_wscale=use_wscale)\n",
    "        \n",
    "    def forward(self,x,latent):\n",
    "        style = self.lin(latent) # style => [batch_size,n_channels*2]\n",
    "        shape = [-1,2,x.size(1)] + (x.dim() - 2) * [1]\n",
    "        style = style.view(shape) # [batch_size,2,n_channels]\n",
    "        x = x * (style[:,0] + 1.) + style[:,1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormLayer(nn.Module): # forcing std for latent vector to be one\n",
    "    def __init__(self,epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self,x):\n",
    "        return x * torch.rsqrt(torch.mean(x**2,dim=1,keepdim=True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurLayer(nn.Module):\n",
    "    def __init__(self,kernel=[1,2,1],normalize=True,flip=False,stride=1):\n",
    "        super(BlurLayer,self).__init__()\n",
    "        kernel = [1,2,1]\n",
    "        kernel = torch.tensor(kernel,dtype=torch.float32)\n",
    "        kernel = kernel[:,None] * kernel[None,:]\n",
    "        kernel = kernel[None,None]\n",
    "        if normalize:\n",
    "            kernel = kernel / kernel.sum()\n",
    "        if flip:\n",
    "            kernel = kernel[:,:,::-1,::-1]\n",
    "        self.register_buffer('kernel',kernel)\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # expand kernels channels\n",
    "        kernel = self.kernel.expand(x.size(1),-1,-1,-1)\n",
    "        x = F.conv2d(x,kernel,stride=self.stride,padding=int((self.kernel.size(2)-1)/2),groups=x.size(1))\n",
    "        return x\n",
    "def upscale2d(x, factor=2, gain=1):\n",
    "    assert x.dim() == 4\n",
    "    if gain != 1:\n",
    "        x = x * gain\n",
    "    if factor != 1:\n",
    "        shape = x.shape\n",
    "        x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n",
    "        x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n",
    "\n",
    "class Upscale2d(nn.Module):\n",
    "    def __init__(self,factor=2,gain=2):\n",
    "        super().__init__()\n",
    "        assert isinstance(factor,int) and factor >= 1\n",
    "        self.gain = gain\n",
    "        self.factor = factor\n",
    "    def forward(self,x):\n",
    "        return upscale2d(x,factor=self.factor,gain=self.gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlin = {'relu': (torch.relu,np.sqrt(2)), 'lrelu':(nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}\n",
    "class G_mapping(nn.Sequential):\n",
    "    def __init__(self,nonlinearity='lrelu',use_wscale=True):\n",
    "        act,gain = nonlin[nonlinearity]\n",
    "        layers = [\n",
    "            ('pixel_norm',PixelNormLayer()),\n",
    "            ('dense0',MyLinear(512,512,gain=gain,lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense0_act',act),\n",
    "            ('dense1',MyLinear(512,512,gain=gain,lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense1_act',act),\n",
    "            ('dense2',MyLinear(512,512,gain=gain,lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense2_act',act),\n",
    "            ('dense3',MyLinear(512,512,gain=gain,lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense3_act',act),\n",
    "            ('dense4',MyLinear(512,512,gain=gain,lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense4_act',act),\n",
    "            ('dense5',MyLinear(512,512,gain=gain,lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense5_act',act),\n",
    "            ('dense6',MyLinear(512,512,gain=gain,lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense6_act',act),\n",
    "            ('dense7',MyLinear(512,512,gain=gain,lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense7_act',act)\n",
    "        ]\n",
    "        super().__init__(OrderedDict(layers))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = super().forward(x)\n",
    "        x = x.unsqueeze(1).expand(-1,18,-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Truncation(nn.Module):\n",
    "    def __init__(self,avg_latent,max_layer=8,threshold=0.7):\n",
    "        super().__init__()\n",
    "        self.max_layer = max_layer\n",
    "        self.threshold = threshold\n",
    "        self.register_buffer('avg_latent',avg_latent)\n",
    "    def forward(self,x):\n",
    "        assert x.dim() == 3\n",
    "        interp = torch.lerp(self.avg_latent,x,self.threshold)\n",
    "        do_trunc = (torch.arang(x.size(1)) < self.max_layer).view(1,-1,1)\n",
    "        return torch.where(do_trunc,interp,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerEpilogue(nn.Module):\n",
    "    \"\"\"Things to do at the end of the layer\"\"\"\n",
    "    def __init__(self,channels,dlatent_size,use_wscale,use_noise,use_pixel_norm,use_instance_norm,use_styles,activation_layer):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if use_noise:\n",
    "            layers.append(('noise',NoiseLayer(channels)))\n",
    "        layers.append(('activation',activation_layer))\n",
    "        if use_pixel_norm:\n",
    "            layers.append(('pixel_norm',PixelNorm()))\n",
    "        if use_instance_norm:\n",
    "            layers.append(('instance_norm',nn.InstanceNorm2d(channels)))\n",
    "        self.top_epi = nn.Sequential(OrderedDict(layers))\n",
    "        if use_styles:\n",
    "            self.style_mod = StyleMod(dlatent_size,channels,use_wscale=use_wscale)\n",
    "        else:\n",
    "            self.style_mod = None\n",
    "    def forward(self,x,dlatents_in_slice=None):\n",
    "        x = self.top_epi(x)\n",
    "        if self.style_mod is not None:\n",
    "            x = self.style_mod(x, dlatents_in_slice)\n",
    "        else:\n",
    "            assert dlatents_in_slice is None\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(nn.Module):\n",
    "    def __init__(self,nf,dlatent_size,const_input_layer,gain,use_wscale,use_noise,use_pixel_norm,use_instance_norm,use_styles,activation_layer):\n",
    "        super().__init__()\n",
    "        self.const_input_layer = const_input_layer\n",
    "        self.nf = nf\n",
    "        if self.const_input_layer:\n",
    "            self.const = nn.Parameter(torch.ones(1,nf,4,4))\n",
    "            self.bias = nn.Parameter(torch.ones(nf))\n",
    "        else:\n",
    "            self.dense = MyLinear(dlatent_size,nf*16,gain=gain/4, use_wscale=use_wscale)\n",
    "        self.epi1 = LayerEpilogue(nf,dlatent_size,use_wscale,use_noise,use_pixel_norm,use_instance_norm,use_styles,activation_layer)\n",
    "        self.conv = MyConv2d(nf,nf,3,gain=gain,use_wscale=use_wscale)\n",
    "        self.epi2 = LayerEpilogue(nf,dlatent_size,use_wscale,use_noise,use_pixel_norm,use_instance_norm,use_styles,activation_layer)\n",
    "    def forward(self,dlatents_in_range):\n",
    "        batch_size = dlatents_in_range.size(0)\n",
    "        if self.const_input_layer:\n",
    "            x = self.const.expand(batch_size,-1,-1,-1)\n",
    "            x = x + self.bias.view(1,-1,1,1)\n",
    "        else:\n",
    "            x = self.dense(dlatents_in_range[:,0]).view(batch_size,self.nf,4,4)\n",
    "        x = self.epi1(x,dlatents_in_range[:,0])\n",
    "        x = self.conv(x)\n",
    "        x = self.epi2(x,dlatents_in_range[:,1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSynthesisBlock(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels,blur_filter,dlatent_size,gain,use_wscale,use_noise,use_pixel_norm,use_instance_norm,use_styles,activation_layer):\n",
    "        super().__init__()\n",
    "        if blur_filter:\n",
    "            blur = BlurLayer(blur_filter)\n",
    "        else:\n",
    "            blur = None\n",
    "        self.conv0_up = MyConv2d(in_channels,out_channels,kernel_size=3,gain=gain,use_wscale=use_wscale,intermediate=blur,upscale=True)\n",
    "        self.epi1 = LayerEpilogue(out_channels,dlatent_size,use_wscale,use_noise,use_pixel_norm,use_instance_norm,use_styles,activation_layer)\n",
    "        self.conv1 = MyConv2d(out_channels,out_channels,kernel_size=3,gain=gain,use_wscale=use_wscale)\n",
    "        self.epi2 = LayerEpilogue(out_channels,dlatent_size,use_wscale,use_noise,use_pixel_norm,use_instance_norm,use_styles, activation_layer)\n",
    "        \n",
    "    def forward(self,x,dlatents_in_range):\n",
    "        x = self.conv0_up(x)\n",
    "        x = self.epi1(x,dlatents_in_range[:,0])\n",
    "        x = self.conv1(x)\n",
    "        x = self.epi2(x,dlatents_in_range[:,1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_synthesis(nn.Module):\n",
    "    def __init__(self,\n",
    "                dlatent_size        = 512,          # Disentangled latent (W) dimensionality.\n",
    "                num_channels        = 3,            # Number of output color channels.\n",
    "                resolution          = 1024,         # Output resolution.\n",
    "                fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
    "                fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
    "                fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
    "                use_styles          = True,         # Enable style inputs?\n",
    "                const_input_layer   = True,         # First layer is a learned constant?\n",
    "                use_noise           = True,         # Enable noise inputs?\n",
    "                randomize_noise     = True,         # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n",
    "                nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu'\n",
    "                use_wscale          = True,         # Enable equalized learning rate?\n",
    "                use_pixel_norm      = False,        # Enable pixelwise feature vector normalization?\n",
    "                use_instance_norm   = True,         # Enable instance normalization?\n",
    "                dtype               = torch.float32,  # Data type to use for activations and outputs.\n",
    "                blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "                ):\n",
    "        super().__init__()\n",
    "        def nf(stage):\n",
    "            return min(int(fmap_base / (2.0 ** (stage**fmap_decay))),fmap_max)\n",
    "        self.dlatent_size = dlatent_size\n",
    "        resolution_log2 = int(np.log2(resolution))\n",
    "        assert resolution == 2**resolution_log2 and resolution >=4\n",
    "        \n",
    "        act,gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "        num_layers = resolution_log2 * 2 - 2\n",
    "        num_styles = num_layers if use_styles else 1\n",
    "        torgbs = []\n",
    "        blocks = []\n",
    "        for res in range(2, resolution_log2 + 1):\n",
    "            channels = nf(res-1)\n",
    "            name = '{s}x{s}'.format(s=2**res)\n",
    "            if res == 2:\n",
    "                blocks.append((name,\n",
    "                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n",
    "                                      use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
    "                \n",
    "            else:\n",
    "                blocks.append((name,\n",
    "                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
    "            last_channels = channels\n",
    "        self.torgb = MyConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale)\n",
    "        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n",
    "        \n",
    "    def forward(self, dlatents_in):\n",
    "        # Input: Disentangled latents (W) [minibatch, num_layers, dlatent_size].\n",
    "        # lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0), trainable=False), dtype)\n",
    "        batch_size = dlatents_in.size(0)       \n",
    "        for i, m in enumerate(self.blocks.values()):\n",
    "            if i == 0:\n",
    "                x = m(dlatents_in[:, 2*i:2*i+2])\n",
    "            else:\n",
    "                x = m(x, dlatents_in[:, 2*i:2*i+2])\n",
    "        rgb = self.torgb(x)\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_all = nn.Sequential(OrderedDict([\n",
    "    ('g_mapping', G_mapping()),\n",
    "    #('truncation', Truncation(avg_latent)),\n",
    "    ('g_synthesis', G_synthesis())    \n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # this can be run to get the weights, but you need the reference implementation and weights\n",
    "    import dnnlib, dnnlib.tflib, pickle, torch, collections\n",
    "    dnnlib.tflib.init_tf()\n",
    "    weights = pickle.load(open('./karras2019stylegan-ffhq-1024x1024.pkl','rb'))\n",
    "    weights_pt = [collections.OrderedDict([(k, torch.from_numpy(v.value().eval())) for k,v in w.trainables.items()]) for w in weights]\n",
    "    torch.save(weights_pt, './karras2019stylegan-ffhq-1024x1024.pt')\n",
    "if 0:\n",
    "    # then on the PyTorch side run\n",
    "    state_G, state_D, state_Gs = torch.load('./karras2019stylegan-ffhq-1024x1024.pt')\n",
    "    def key_translate(k):\n",
    "        k = k.lower().split('/')\n",
    "        if k[0] == 'g_synthesis':\n",
    "            if not k[1].startswith('torgb'):\n",
    "                k.insert(1, 'blocks')\n",
    "            k = '.'.join(k)\n",
    "            k = (k.replace('const.const','const').replace('const.bias','bias').replace('const.stylemod','epi1.style_mod.lin')\n",
    "                  .replace('const.noise.weight','epi1.top_epi.noise.weight')\n",
    "                  .replace('conv.noise.weight','epi2.top_epi.noise.weight')\n",
    "                  .replace('conv.stylemod','epi2.style_mod.lin')\n",
    "                  .replace('conv0_up.noise.weight', 'epi1.top_epi.noise.weight')\n",
    "                  .replace('conv0_up.stylemod','epi1.style_mod.lin')\n",
    "                  .replace('conv1.noise.weight', 'epi2.top_epi.noise.weight')\n",
    "                  .replace('conv1.stylemod','epi2.style_mod.lin')\n",
    "                  .replace('torgb_lod0','torgb'))\n",
    "        else:\n",
    "            k = '.'.join(k)\n",
    "        return k\n",
    "\n",
    "    def weight_translate(k, w):\n",
    "        k = key_translate(k)\n",
    "        if k.endswith('.weight'):\n",
    "            if w.dim() == 2:\n",
    "                w = w.t()\n",
    "            elif w.dim() == 1:\n",
    "                pass\n",
    "            else:\n",
    "                assert w.dim() == 4\n",
    "                w = w.permute(3, 2, 0, 1)\n",
    "        return w\n",
    "\n",
    "    # we delete the useless torgb filters\n",
    "    param_dict = {key_translate(k) : weight_translate(k, v) for k,v in state_Gs.items() if 'torgb_lod' not in key_translate(k)}\n",
    "    if 1:\n",
    "        sd_shapes = {k : v.shape for k,v in g_all.state_dict().items()}\n",
    "        param_shapes = {k : v.shape for k,v in param_dict.items() }\n",
    "\n",
    "        for k in list(sd_shapes)+list(param_shapes):\n",
    "            pds = param_shapes.get(k)\n",
    "            sds = sd_shapes.get(k)\n",
    "            if pds is None:\n",
    "                print (\"sd only\", k, sds)\n",
    "            elif sds is None:\n",
    "                print (\"pd only\", k, pds)\n",
    "            elif sds != pds:\n",
    "                print (\"mismatch!\", k, pds, sds)\n",
    "\n",
    "    g_all.load_state_dict(param_dict, strict=False) # needed for the blur kernels\n",
    "    torch.save(g_all.state_dict(), './karras2019stylegan-ffhq-1024x1024.for_g_all.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_all.load_state_dict(torch.load('./karras2019stylegan-ffhq-1024x1024.for_g_all.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0204687a3a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlatents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;31m# normalization to 0..1 range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-99e29a812c74>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dlatents_in)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlatents_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdlatents_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-3a7d910d9197>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, dlatents_in_range)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlatents_in_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepi1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlatents_in_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d6f6debac481>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_mul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave_convolution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_mul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "import torchvision\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "g_all.eval()\n",
    "g_all.to(device)\n",
    "\n",
    "torch.manual_seed(20)\n",
    "nb_rows = 2\n",
    "nb_cols = 5\n",
    "nb_samples = nb_rows * nb_cols\n",
    "latents = torch.randn(nb_samples, 512, device=device)\n",
    "with torch.no_grad():\n",
    "    imgs = g_all(latents)\n",
    "    imgs = (imgs.clamp(-1, 1) + 1) / 2.0 # normalization to 0..1 range\n",
    "imgs = imgs.cpu()\n",
    "\n",
    "imgs = torchvision.utils.make_grid(imgs, nrow=nb_cols)\n",
    "\n",
    "pyplot.figure(figsize=(15, 6))\n",
    "pyplot.imshow(imgs.permute(1, 2, 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
